% !TEX root = /home/frank/School/thesis_text/thesis.tex



\chapter{Platform Overview}

\section{Zynq-7000}

The Zynq-7000 System on Chip combines a dual core ARM Cortex-A9 with Xilinx programmable logic in a single device. This combination of a CPU and an FPGA on the same device is not a new phenomenon, with examples of previous generations being the PowerPC based Xilinx Virtex-II Pro and some models of the Virtex 4 and Virtex 5 series FPGA's. The two most notable differences between these generations is the shift from PowerPC based architectures to ARM based architectures, and a notable shift in emphasis from HDL centered design to a more programmer centric view with an emphasis on high level languages. 
	

	\subsection{Processing System}
	The Zynq-7000 series SoC is split into two parts: The processing system (PS) and the programmable logic (PL). The Processing system (PS) contains an Application Processor Unit (APU), memory interfaces and I/O peripherals. 
	
		\paragraph{APU}
		The APU is a Dual ARM Cortex-A9 CPU which implements version 7 of the ARM ISA  as well as Thumb and Jazelle instruction sets. Each core has a NEON Media Processing Engine supporting SIMD vector and scalar single-precision floating-point and integer computation and scalar double-precision floating-point computation. Each core has 32 KB instruction and 32 KB data caches and there is 512 KB shared L2 cache and 256 KB of shard on-chip SRAM memory. The APU also has a snoop control unit to maintain L1 and L2 coherency. This snoop control unit also controls the Accelerator Coherency Port, a 64-bit AXI slave port from the programmable logic, which performs the role of master, to the processing system which serves as slave. This allows direct communication between the PS and the PL through the L2 caches or on chip memory with guaranteed coherency. The also has an on-board 8-channel DMA controller with 4-channels reserved for PS to/from memory and 4 for PL to/from memory transers. Finally the Processing system also contains an interrupt controller.

		\paragraph{Memory Controller}
		The Memory controller supports a number of memory technologies. The system has a DDR controller which supports DDR2 and DDR3 memory, a Quad-SPI controller which converts normal memory read operations to SPI and vice versa, and a Static Memory Controller which supports NAND and SRAM/NOR type memory.

		\paragraph{I/O Peripherals}
		The Processing system contains quite a lot of industry standard I/O peripherals for external data communication.
			\begin{multicols}{2}
				\begin{itemize}
					\item GPIO
					\item 2 Gigabit Ethernet Controllers
					\item 2 USB controllers
					\item 2 SD/SDIO controllers
					\item 2 SPI controllers
					\item 2 CAN controllers
					\item 2 UART controllers
					\item 2 I$^{2}$C controllers
				\end{itemize}
			\end{multicols}

		These peripherals are connected to multiplexed I/O buffers which enable to externalize these signals to up to 54 pins. If there is a need for more I/O pins the signals can be routed into the PL through the extended MIO, where they can be routed directly to package pins or peripherals in the PL.

	\subsection{Programmable Logic}
	The programmable logic provides the same functionality that can be expected from a Xilinx FPGA. The PL in 7z010 and 7z020 Zynq SoCs is based on Artix-7 FPGAs whereas the PL in 7z030, 7z045 and 7z100 SoCs is based on Kintex-7 FPGA logic. This PL can be coupled through a couple of different interconnects, with varying degrees of interconnectedness between the PL and the PS. Of note here is that the PS has to be booted first and the PL logic has to be configured from the PL at boot or at a later time. This is another example of the shift to a more software centered view. The system has all the features one can expect from an FPGA: configurable logic blocks with look-up tables, a number of 36 KB block RAMs, DSP348E slices and configurable IO. The PL side also contains an Analog to Digital converter, and in the larger varieties of the Zynq SoC an integrated PCI Express block.

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{/home/frank/School/thesis_text/images/zynq_block_diagram.png}
\caption{Zynq -7000 SoC overview \cite{anon._zynq-7000_2013}}
\label{img:zynq_overview}
\end{figure}

	\subsection{Interconnect}
	The interconnect system is located in the PS but because of it's influence on performance it warrants its own section. The interconnect system is comprised of a number of switches to connect the different parts of the system using the AXI point-to-point protocol. The AXI protocol is part of the ARM Advanced Microcontroller Bus Architecture version 3.0. These AXI interconnects are the primary means of communication between the PS and the PL. There are a number of different interface ports between the PS and the PL:

		\begin{description}
			\item[AXI\_HP] There are four AXI\_HP interfaces, connecting PL masters with high bandwidth datapaths to the DDR and OCM memories. Each interface is buffered with 2 FIFOs and is configurable to be 32 or 64 bits wide.
			\item[AXI\_GP] The four general purpose AXI\_GP ports are divided into 2 master ports and 2 slave ports. These ports don't have FIFO buffering which makes them less suitable for high performance use. 
			\item[AXI\_ACP] The Accelerator coherency port is a 64-bit AXI slave interface that directly connects the PL to the APU caches. This is done through the snoop control unit and can enforce coherency if requested. 
		\end{description}

	The actual interconnection is done through a number of switches. Amongst these are the snoop control unit, the L2 cache controller and a couple of ARM NIC-301 based interconnect switches.

		\begin{description}
			\item[Snoop Control Unit] Although the SCU is in essence not a switch, its behavior in regards to the transfer of data from its AXI slave ports to its AXI master ports makes it function as a switch.
			\item[Central Interconnect] The central interconnect is the core of the interconnect network in the Zynq SoC.\
			\item[Master Interconnect] The master interconnect connects the Master switches the traffic from the AXI\_GP ports as well as traffic coming from the device configuration core and the debug access port.
			\item[Slave Interconnect] The slave interconnect switches traffic comming from the central interconnect to AXI\_GP, I/O peripherals, APB connections, etc.
			\item[Memory Interconnect] The memory interconnect switches high speed traffic comming from the AXI\_HP ports to DDR and on-chip RAM.
			\item[OCM Interconnect] The on-chip memory connect switches the traffic from the central interconnect and the memory interconnect.
		\end{description}

		A diagram of the way these interconnects are organized can be found in figure \ref{img:zynq_intereconnect}

\begin{figure}[H]
\centering
\includegraphics[scale=0.55]{./images/interconnect_block_diagram.png}
\caption{Zynq Interconnect System Block Diagram \cite{anon._zynq-7000_2013}}
\label{img:zynq_intereconnect}
\end{figure}

\section{Toolchain}



\section(Targeted Reference Design 14.5)


